---
title: "Machine learning proyect: \nUsage of Random Forest to predict the classe variable with PCA"
author: "Crist칩bal Morgado"
date: "`r Sys.Date()`"
output: html_document
---

```{r options, echo = F, warning = F, message = F }
knitr::opts_chunk$set(echo = T, warning = F, message = F)
```

```{r packages}
library(tidyverse)
library(caret)
library(doParallel)
library(parallel)
library(kableExtra)
```

# 1: Introduction

This report aim is to create a ML model that correctly classify the "classe" variable in a data set based on the performance of 6 different subjects in barbell lifts exercises.

In order to achieve that, first we eliminate variables with more than 90% missing values, since they are functions of the main variables and don't offer much information to the model. The data also has been splitted into training, validation and test set.

We used Principal Component Analysis (PCA) to reduce the variables to the ones that explain the 95% of the variance in the training set, with this new variables, generated by PCA, we perform a Random Forest model and we tested it with the validation set. Using a confusion matrix we can estimate the out-of-sample error, calculated as 1-accuracy in the validation set.

Finally we used this model to predict the Classe of the 20 observations in the test set.

For this assignment I've used the following version of R and caret package.

**R version 4.2.2 **

**caret_6.0-93 **

**Seed = "1234"**

# 2: Feature selection and modelling

## 2.1 - Feature selection
Since the original training set has 160 variables, and I've doesn't really understood them, my approach to generate the model was to perform a PCA and use the new variables generated by this process to estimate the model. 

First I've drop the variables with more than 80% NA rate, since they are giving no actual information to the model. I also have drooped the first 7 variables of the data set, since they are mostly metadata.

Then I create a validation set to estimate the out-of-sample error in the next steps. 

```{r load the data and drop missing variables}
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")

retirar <- data.frame(row.names = c("numero","si_no"))
for(i in 1:dim(training)[2]){
  t <- as.numeric(table(is.na(training[,i]))["TRUE"])
  f <- as.numeric(table(is.na(training[,i]))["FALSE"])

  na_percent <- t/(t+f)
  

  if(is.na(na_percent)){
    retirar[i,1] <- i
    retirar[i,2] <- FALSE
  } else if(na_percent >= 0.8){
    retirar[i,1] <- i
    retirar[i,2] <- TRUE
  } else {
    retirar[i,1] <- i
    retirar[i,2] <- FALSE
  }
}
retirar <- retirar[retirar$V2 == TRUE,1]
train1 <- training[,-retirar]

# separar el set de entrenamiento y crear un set de validaci칩n, con el objetivo de obtener un estimado del error fuera de la muestra
set.seed(1234)
index <- createDataPartition(training$classe,
                             p=0.7,
                             list = F)
validation <- train1[-index,]
train1 <- train1[index, ]
train1 <- train1[,-c(1:7, 93)]
```

## 2.2- PCA

The next step was to perform a Principal Component Analisys and use this variables to perform the model. I've set the preProces() function to get the 95% of variance, this create 25 PC. Finally we add the classe variable to the dataset and prepare to perform the model.
```{r PCA}
# retirar primeras 7 variables pues no entregan informaci칩n relevante
# adem치s se quita la variable classe para calcular los PCA
# finalmente se calculan los PC
train1 <- train1[,-c(1:7, 93)]
set.seed(1234)
pca1 <- preProcess(train1, method = "pca")
training_pca <- predict(pca1, train1)
training_pca <- training_pca[,grepl("^PC", names(training_pca))]
training_pca$classe <- training[index, "classe"] # se agrega la variable dependiente
```

## 2.3- Modelling

Following the tip from the blog "[ Improving Performance of Random Forest with caret::train()](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md)" I've decided to use a Random Forest model to classify the data. 

Here I've used the trainControl() function to set the parameter wich the model should use. I've used K-fold cross-validation since is faster than the default method (bootstrap) with 5 folds, and used the doParallel and parallel packages to perform the model with the multiple cores of the processor. 

```{r modelling, cache=TRUE}
cluster <- makeCluster(detectCores()-1) # se deja uno libre para que el computador funcione
registerDoParallel(cluster)

fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = T) # este objeto entrega a train() las preferencias con que debe calcular el modelo

set.seed(1234)
model_1 <- train(classe~.,
                 data = training_pca,
                 method="rf",
                 trControl = fitControl)

stopCluster(cluster)
registerDoSEQ() 
```

# 3: Estimated results
## 3.1- Cross-Validation

For this project I've created a validation set with the createDataPartition() function, splitting the original training data. The training set contains 70% of observations and the validation set 30%. In this new set I've predict the results and use the confusionMatrix() function to estimate the out-of-sample error.

## 3.2- Out-of-sample error

```{r}
validation_pca <- predict(pca1, validation)
pred_validation <- predict(model_1, validation_pca)

aux_2 <- factor(training[-index, "classe"])
conf_val <- confusionMatrix(pred_validation, aux_2)
```
I've used the validation test to estimate the out of sample error, the model has an estimated error of `r 1- conf_val[["overall"]]["Accuracy"]`.

```{r}
conf_val$table %>% 
  kableExtra::kbl(caption = "Confussion Matrix") %>% 
  kableExtra::kable_classic_2(lightable_options = c("striped", "hover"))
```

```{r, results='markup'}

conf_val$overall %>% 
  kableExtra::kbl(caption = "Overall Results") %>% 
  kableExtra::kable_classic_2(lightable_options = c("striped", "hover"))
```

```{r results='markup'}
conf_val$byClass %>% 
  kableExtra::kbl(caption = "Results by Class") %>% 
  kableExtra::kable_classic_2(lightable_options = c("striped", "hover"))
```


# 3.3- Predictions of the test set
We finally use this model to predict the values from the test set. in order to do that we make the same transformations of the data set we did in the training and validation set and apply the same PCA of the training set to create the new variables in the test set.  
```{r}
retirar <- data.frame("numero", "si_no")
for(i in 1:dim(testing)[2]){
  t <- as.numeric(table(is.na(testing[,i]))["TRUE"])
  f <- as.numeric(table(is.na(testing[,i]))["FALSE"])
  
  na_percent <- t/sum(t,f, na.rm = T)
  
  
  if(is.na(na_percent)){
    retirar[i,1] <- i
    retirar[i,2] <- FALSE
  } else if(na_percent >= 0.8){
    retirar[i,1] <- i
    retirar[i,2] <- TRUE
  } else {
    retirar[i,1] <- i
    retirar[i,2] <- FALSE
  }
}
retirar <- retirar[retirar$X.si_no. == TRUE,1]


test2 <- testing[,-as.numeric(retirar)]
test2 <- test2[,-c(1:7, 60)]
testing_pca <- predict(pca1, test2)

pred_test <- predict(model_1, testing_pca)
```

After the pre Process, we run the model and get the estimations for the test set. The results are shown in the next chunk of code.
```{r}
pred_test
```


